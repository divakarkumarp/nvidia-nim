# nvidia-nim (NVIDIA Inference Module)

![_28b08283-31a7-47be-a7ea-8dd3caffb92a](https://github.com/divakarkumarp/nvidia-nim/assets/32620288/814579c0-48d8-4d86-943c-2fdb62ae3f2a)

Nvidia NIM (Neural Inference Microservices) enhances AI model deployment by offering optimized inference engines tailored to various hardware configurations, ensuring low latency and high throughput. Part of the Nvidia AI Enterprise suite, NIM supports a wide array of AI models and integrates seamlessly with major cloud platforms like AWS, Google Cloud, and Azure​ (NVIDIA Newsroom)​​ (NVIDIA Investor Relations)​. It is used across industries for applications ranging from generative AI and drug discovery to customer service optimization​ (NVIDIA Investor Relations)​​ (NVIDIA)​. Developers can access and deploy NIM microservices easily, with support for popular AI frameworks and tools, facilitating scalable and efficient AI application deployment​ (NVIDIA Developer)​​ (NVIDIA)​.

![image](https://github.com/divakarkumarp/nvidia-nim/assets/32620288/e574a0be-b228-4c34-9429-757983616963)
### Technologies Used:

![](https://forthebadge.com/images/badges/made-with-python.svg)

[<img target="_blank" src="https://github.com/divakarkumarp/Building-Chatboat-with-Langchain-Ollama/assets/32620288/4ab426a3-6311-4a2f-a7a7-b4e58840489f" width=100>](https://ollama.com/)       [<img target="_blank" src="https://github.com/divakarkumar424/Text-To-SQL-LLM-App/assets/32620288/a9f2554b-0e9d-45e1-af4f-9f7b08d5bffd" width=100>](https://streamlit.io/)   [<img target="_blank" src="https://github.com/divakarkumar424/Langchain-POC/assets/32620288/564a6dac-9ee4-4d64-9629-abcfc4af1ea1" width=100>](https://www.langchain.com/)
